# Extract, Transform and Loading

## ETL and its need
Extract, Transform and Load(ETL) is a proecess of *data integration* that encompasses three steps: ***Extraction***, ***Trnasformation***, ***Loading***

- **Extract:** This stage determines *which data sources to use, the refresh rate of each source and the priorities between them.*
- **Transform:** It brings clarity and order
  - Date and times :- combined into a single format
  - String parse down into their underlying meanings.
  - Location data converted to coordinates, zip codes or cities/countries.
  - sums up, rounds and average measures
  - deletes useless data and errors
  - masks personally identifiable info to comply with GDPR, CCPA and other privacy requirements
- **Loading:**
  - determines targets and refresh rates
  - This phase determines whether loading will happen incrementally or it will require updating existing data and inserting new data.

![image](https://github.com/GautamRaj-12/ignou-mca_new-notes/assets/64408989/60f1b98f-9a78-4af4-9fb7-ecb141a12771)

## ETL Process
3 steps:
1. **Data Extraction**
   - 4 Steps
     1. **Identify the data to extract:** From various sources like SQL or NOSQL DB or any other platform - which data you want , which data field is required
     2. **Estimate how large the data extraction is:** make large datasets manageable by dividing or upgrade hardware to handle larger dataset.
     3. **Choose the extraction method:**
        - 3 principles
          1. **Update Notifications:** Preferred method, notification sent that record has changed, update only new info.
          2. **Incremental Extraction:** Identify which records have changed and perform extractio of only those records.
          3. **Full Extraction:** Complete Update, Only feasible to small datasets.
     4. **Assess your SaaS Platform:**
        - formerly in-house warehouse
        - now on site server like Google Analytics, Hubspot, Salesforce etc.
2. **Data Transformation**
   - **Deduplication(Normalizing):** Remove duplicate info
   - **Key Restructuring:** Draws key connections from one table to another
   - **Cleansing:** Delete old, incomplete, duplicate data
   - **Format Revision:** Converts data in diff. format to single consistent format
   - **Derivation:** Subtract/add - deriving data
   - **Aggregation:** Gathers abd searches data - summarized repeat format
   - **Integration:** Reconciles diverse names/values to standard
   - **Filtering:** Selects specific columns,rows and fields
   - **Splitting:** Splits one column into more than one column
   - **Joining:** Links data from two or more sources
   - **Summarization:** Creates different business metrics by calculating
   - **Validation:** Rules to follow in diff. circumstances
3. **Data Loading**
   - Processs of loading the extracted info into the target repository
   - 2 ways
     1. **Full Load**
     2. **Incremental Load:** Extract abd Load infor that has appeared since last load
   - **Types of Incremental Loads**
     1. **Batch Incremental Loads**
        - info in packets/batches
        - if large patch - at off peak hours
        - small batch - on a minute by minute basis, good for real time updates
     2. **Streaming Incremental Loads**
        - ingests new data as it appears in real time
        - only possible when the updates involve a very small amount of data
   - **Challenges in Incremental Loading**
     - **Data Structure Changes:** One part of the system changes ➡ lead to incompatibilities that interfere with loading process ➡ leading to inconsistent and corrupt data
     - **Processing data in the wrong order:** Data pipelines ➡ Follow complex trajectories ➡ results in data warehouse processing, updating or deleting info in wrong order.
     - **Failure to detect problems:** API going down, API access credentials ➡ out of date ➡ should be detected quickly

## Working of ETL
1. **Parsing/Cleansing:** Data generated by apps ➡ JSON/XML/CSV ➡ parse it into tables ➡ extra specific fields
2. **Data Enrichment:** Prep. data for analytics ➡ injecting expert knowledge, resolving discrepancies and correcting bugs.
3. **Setting velocity:** Velocity ➡ refers to freq. of data loading
4. **Data Validation:** ETL finds if data is empty, corrupted or missing crucial elements ➡ determines ➡ stop the entire process, skip the data or set data aside

## Layered Implementation of ETL in a data warehouse
1. **Mirror/Raw Layer:** Copy of source files/tables ➡ no logic or enrichment
2. **Staging layer:** Transform ➡ holds the final form of the data for incremental part
3. **Schema Layer:** Destination tables, which contain all data after cleansing, enrichment and transformation
4. **Aggregating Layer:** Aggregate data to daily/store level. Improves report performance, easy addition of business logic etc.

## ETL and OLAP Data Warehouses
- ETL + OLAP ➡ Easier data analysis, excellent high speed reading and analysis
- During ETL Info is:
  1. Extracted from various RDBMS/OLTP
  2. Transformed within a staging area
  3. Loaded into OLAP DW Server

## ETL and their benefits
- **Scalability:** Scaling hand-coded ETL solutions ➡ complex. ***Xplenty*** ➡ Unlimited Scalability ➡ establishes solid pipelines, deploys necessary resources
- **Simplicity:** All our needs layered into one tool saves our time and resources.
- **Out-of-the-box:** Open-source tools ➡ ***Apache Airflow*** ➡ requires customization while Xplenty works out of the box
- **Compliance:** GDPR, CCPA, HIPPA, privacy nets ➡ easy when using tools
- **Long term costs:** Cloud based tools ➡ Cheaper in long term, hand coded ➡ cheaper initially but maintenance complex and expensive in long term

## Improving the importance of ETL
- **Tackle the bottlenecks:** Make sure to track hardware usage, check resources used by each part ➡ tackle the issues
- **Load data incrementally:** Difficult to implement ➡ but very efficient
- **Partition large tables:** Big tables ➡ smaller ones ➡ each partition has index and index tree ➡ quicker access to data
- **Cut out extraneous data:** Define exactly which data should be processed and leave irrelevant rows/columns
- **Cache the data:** Speeds up things
- **Process in parallel using Hadoop:** ***Apache Hadoop*** ➡ designed for distributed for processing of large data over a cluster of machines. It uses HDFS that cuts data into small chunks. Two stages: 1. Map 2. Reduce

## ELT and its need
- Alternative to ETL, It pushes the transformation component of the process to target database for better performance.
- Extraction from various sources ➡ Load into DW ➡ Transform at target DW

## ELT Process
![image](https://github.com/GautamRaj-12/ignou-mca_new-notes/assets/64408989/17a7427c-aeb2-4527-8af8-0c77d6bca5de)

## Need of ELT
- Effective for
  - Large enterprises with vast data volumes
  - businesses that collects data from various sources/dissimilar formats
  - quick or frequent access
  - low maintenance solution'

## Benefits of ELT
- Authorized Users ➡ securely access data without returning to source systems
- No downloading required
- Better results with more effecient effort
- Transform data faster
- Combine data from diff. sources and formats
- Manage data at scale
- Save time and money

## ETL vs ELT

| Aspect              | ETL                                       | ELT                                                       |
|---------------------|-------------------------------------------|-----------------------------------------------------------|
| **Load time**       | Longer, as data is extracted before loading| Shorter, as data is loaded first and then transformed      |
| **Transformation time** | Transformation occurs before loading  | On-demand transformation using target system's computing  |
| **Complexity**      | Typically user-friendly GUI               | Requires in-depth BI tool knowledge and database skills   |
| **Data warehouse support** | Better fit for legacy on-premise data warehouses and structured data | Designed for scalability in the cloud                      |
| **Maintenance**     | Requires significant maintenance for updating data in the data warehouse | Data is always available in near real-time, less maintenance |
